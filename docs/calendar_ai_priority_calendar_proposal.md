# 📅 Calendar AI – Priority Calendar  
**Post‑Training Proposal (Version 1.0 – Oct 2025)**  
Prepared for: Rajesh (SLT – AI Strategy)  
Co‑sponsors: Pratibha (Product – Calendar), Dongmei (Engineering – ML Ops)  
Owner: Chin‑Yew (Calendar AI Lead)

---

## 1. Executive Summary  
Priority Calendar is a personalized, context‑aware assistant that surfaces the right items at the right time—emails, meetings, tasks, and external events—so that every user can focus on what truly matters.

### Why it matters now  
| Business Need | Current Gap | Priority‑Calendar Value |
|---------------|-------------|------------------------|
| Information overload – 70 % of knowledge workers spend >2 h/day triaging Outlook | Prompt‑only Copilot can surface items but cannot rank them by personal/organizational priority | Reduces manual triage → +15 % net productive time per user |
| Decision‑fatigue – meetings often contain low‑value items | No persistent model of a user’s “priority pattern” across email, calendar, Teams, Planner | Proactive time‑blocking of high‑impact work → +8 % meeting‑prep efficiency |
| Trust & Adoption – users are skeptical of “black‑box” suggestions | Copilot prompts are opaque; users can’t see why something is shown | AI‑native UX with explainable ranking, feedback loops, and privacy‑by‑design builds confidence and drives adoption |

The proposal directly satisfies Rajesh’s three critical asks while incorporating user‑trust mechanisms and an AI‑native, learning‑in‑the‑flow UX that continuously improves from each interaction.

---

## 2. Alignment with Rajesh’s Framework  

| Rajesh Ask | How Priority Calendar Answers It |
|------------|----------------------------------|
| **1️⃣ Thesis – Remediation/Differentiation** | *Remediation:* Eliminates the manual “Inbox‑and‑Calendar triage” bottleneck.<br>*Differentiation:* Introduces a personal priority model (PPM) that fuses calendar, mail, task, and organization‑wide signals, fine‑tuned on real user behavior—not just generic LLM prompting. |
| **2️⃣ GPU Constraints & Resource Plan** | Phased, cost‑aware rollout (see §4).<br>Phase 1 uses LLM‑assisted labeling + lightweight ranking model (≈0.8 GPU‑hours per 10 K items).<br>Phase 2 adds a tiny fine‑tuned transformer (≈200 M params) for organization‑wide patterns, run on existing Azure AI Inference capacity (≤ 2 GPUs per region). |
| **3️⃣ Scenario Definition & Post‑Training Ask** | *Scenario:* Priority Calendar – predict top‑N items to surface in the “Focused view” of Outlook/Teams/Planner each morning and when a user opens the day view.<br>*Post‑training needed:*<br>• Supervised fine‑tuning of a ranking model on user‑labelled priority data (≈100 K examples).<br>• Reinforcement‑learning from explicit feedback (thumbs‑up/down, “snooze”, “move to later”).<br>• Optional RLHF for the “explainability” layer that generates concise “why this matters” snippets. |

---

## 3. Core Innovation – AI‑Native, Trust‑Centric UX  

| UX Element | What It Does | Trust Benefit |
|------------|--------------|---------------|
| **1️⃣ Continuous Preference Capture** | Implicit signals (open‑time, dwell, reschedule) + explicit feedback (thumbs, “Not a priority”) are streamed to the Preference Service. | Users see their own actions directly shaping recommendations. |
| **2️⃣ Explainable Ranking** | For each surfaced item a one‑sentence rationale (“Because you met with X yesterday and Y project is due in 2 days”). Generated by a tiny LLM (≈30 M params) fine‑tuned on explanation data. | Transparency reduces perceived “black‑box” risk. |
| **3️⃣ Privacy‑by‑Design Data Pipeline** | All signals are hashed & scoped to the user’s tenant; no raw email text leaves the compliance boundary. Model training uses Federated Learning for organization‑wide patterns, never centralizing raw content. | Meets GDPR, C3PA, and internal policy → higher user confidence. |
| **4️⃣ Adaptive Confidence UI** | Items with low confidence are shown with a “review?” badge; high‑confidence items appear bold. Users can promote/demote items, instantly influencing the model. | Gives users control and visible impact, fostering trust. |
| **5️⃣ “Undo‑Learn” Guardrail** | If a user repeatedly rejects a suggestion, the system auto‑discounts the underlying feature weight for that user. | Prevents “model drift” that would erode trust. |

Result: A closed feedback loop where every interaction both helps the user and feeds the model, delivering a living, trustworthy assistant.

---

## 4. Resource & GPU Plan  

### 4.1 Phased Delivery  

| Phase | Goal | Model(s) | GPU Hours (est.) | Data Requirement | Timeline |
|-------|------|----------|------------------|------------------|----------|
| **P0 – Discovery & Labeling** | Build a high‑quality labelled set (priority vs non‑priority). Use LLM‑assisted labeling(Claude‑2 / Mistral‑7B) to accelerate. | Labeling LLM (inference only) | 0.1 GPU‑hr / 1 K items (≈10 GPU‑hrs total) | 30 K user‑curated examples (via pilot). | 2 mo |
| **P1 – Lightweight Ranking (M1)** | Deploy a gradient‑boosted tree / XGBoost model with engineered features (time‑of‑day, sender‑rank, project tags). No GPU at inference. | CPU‑only scoring; fine‑tune on Azure ML (GPU for training). | 0.8 GPU‑hr / 10 K items (≈8 GPU‑hrs total). | 100 K labelled rows (augmented via synthetic data). | 3 mo |
| **P2 – Transformer Ranker (M2)** | Introduce a 200 M‑parameter transformer fine‑tuned on the same data to capture deeper semantic patterns. | Inference on Azure AI Inference(2 GPUs per region, < 5 % utilization). | 2 GPU‑hrs / 10 K items (≈20 GPU‑hrs total). | Same 100 K + 50 K new labels from Phase P1 feedback. | 4 mo |
| **P3 – RLHF Explainability Layer** | Fine‑tune a 30 M‑parameter LLM to generate concise “why” statements, using RLHF from user thumbs‑up/down on explanations. | Small LLM on Azure OpenAI (GPU inference < 1 GPU). | 1 GPU‑hr / 20 K explanation pairs. | 20 K explanation pairs. | 30 days |
| **P4 – Federated Org‑wide Learning** | Set up secure weight‑aggregation service; run first cross‑tenant update; validate privacy guarantees. | Secure weight‑aggregation service. | Spot VMs & cost‑effective bursts; cap training to 10 % of monthly quota. | No raw data leaves tenant. | 4 mo |

*Total GPU budget for Year 1 ≈ 45 GPU‑hours (well under the < 5 % capacity constraint).*

### 4.2 Compliance & Cost Safeguards  

- Dedicated GPU quota: **≤ 2 GPUs per region** locked for the entire lifecycle.  
- Spot VMs for training bursts (cost‑effective, but capped to 10 % of the monthly quota).  
- Continuous privacy‑impact assessments (PIA) and quarterly internal audit.  

---

## 5. Governance & Cross‑Team Collaboration  

| Governance Body | Frequency | Decision Scope | Representative |
|-----------------|-----------|----------------|----------------|
| **Steering Committee** (Rajesh, Pratibha, Dongmei, Gaurav, Legal) | Monthly | Budget, scope changes, risk escalation | SLT |
| **Technical Review Board** (ML Engineers, Data Scientists, Security) | Bi‑weekly | Model architecture, GPU allocation, privacy safeguards | Engineering |
| **User‑Trust Council** (UX Researchers, Accessibility, Customer Success) | Monthly | UI/UX guidelines, explainability standards, feedback loop design | Product |
| **Compliance & Privacy Office** | Quarterly | Data‑handling policies, audit outcomes | Legal/Compliance |

### RACI Matrix (high‑level)  

| Activity | R | A | C | I |
|----------|---|---|---|---|
| Data collection & labeling | Qingwei | Chin‑Yew | Shi, Legal | Pratibha |
| Model training & GPU budgeting | Gaurav | Dongmei | Rajesh, Finance | Pratibha |
| Explainability UI design | Pratibha | Chin‑Yew | UX Research, Legal | Rajesh |
| Federated learning deployment | Dongmei | Gaurav | Security, Compliance | Pratibha |
| Pilot rollout & feedback analysis | Chin‑Yew | Pratibha | Customer Success | Rajesh |

---

## 5. ROI & Business Impact  

| KPI | Baseline (Q4 2024) | Target (Q4 2025) | Incremental Value |
|-----|--------------------|------------------|-------------------|
| Average productive time per user | 5 h/day (post‑meeting) | 5 h + 15 min | +$12 M (2 M users, $100 / user yr) |
| Outlook triage clicks | 30 clicks/shift | 22 clicks/shift | +$4 M |
| Adoption of Copilot features | 45 % | 70 % | +$8 M |
| GPU cost | $0 | $0.03 / active‑user / mo | $0.72 M annual |
| Trust/CSAT | 72 % satisfied | ≥ 85 % | Lower churn, higher NPS (+0.6 points) |

**Total incremental ARR ≈ $24 M for FY‑26**, with negligible GPU OPEX increase.

---

## 6. Risk Management & Mitigation  

| Risk | Impact | Mitigation |
|------|--------|------------|
| Model drift / loss of relevance | Users stop trusting the assistant | Continuous feedback loop; automatic confidence decay for repeatedly rejected items. |
| GPU overload during training spikes | Delayed releases | Phase‑gated training; use Spot VMs for cost‑effective GPU bursts; cap training to 10 % of monthly quota. |
| Privacy/regulatory breach | Legal & reputational damage | • All raw content stays inside the tenant’s compliance boundary.<br>• Federated‑learning aggregation uses Homomorphic Encryption for weight updates.<br>• Periodic privacy‑impact assessments (PIA) with Legal & Compliance.<br>• Opt‑out toggle for users who want to disable Priority‑Calendar. |
| Explainability layer generates inaccurate “why” statements | Users distrust the system | • Train on 20 K curated pairs.<br>• Confidence threshold: low‑confidence explanations are hidden (“Reason unavailable”).<br>• Explicit “Explanation incorrect” feedback triggers rapid re‑training. |
| Data sparsity for new hires / low‑activity users | Model over‑prioritizes generic items | Cold‑start hybrid: combine federated priors + rule‑engine (e.g., manager‑flagged items).<br>Active‑learning prompts (“Is this a priority for you?”) in first 5 days. |
| GPU contention with other Copilot initiatives | Delays for other projects | Dedicated GPU quota (≤ 2 GPUs per region). <br>Schedule heavy fine‑tuning during off‑peak windows. |
| Change‑management resistance from IT admins | Slow rollout | Admin‑control panel with ROI dashboards and gradual‑rollout switch (10 % → 30 % → 100 %).<br>Pilot‑to‑enterprise migration playbook with success metrics. |

---

## 7. Implementation Timeline & Milestones  

| Sprint (2‑wk) | Deliverable | Owner(s) | Acceptance Criteria |
|---------------|------------|----------|---------------------|
| **S0 (Weeks 1‑2)** | Project charter, governance board, data‑privacy checklist | PM, Legal, Dongmei | Signed charter, risk register approved |
| **S1‑S4 (Weeks 3‑10)** | Phase P0 – Data collection & LLM‑assisted labeling<br>• Build labeling UI<br>• Deploy auto‑labeler<br>• Gather 30 K high‑confidence labels | Qingwei (LLM), Shi (data ops), Pratibha (UX) | ≥ 30 K labeled items, 85 % auto‑label accuracy |
| **S5‑S8 (Weeks 11‑18)** | Phase P1 – XGBoost ranking MVP<br>• Feature engineering<br>• Model training & offline evaluation<br>• Integrate into Outlook “Focused view” (beta) | Gaurav (ML), Dongmei (ML‑Ops) | Top‑3 precision ≥ 0.78 on held‑out set, UI latency < 200 ms |
| **S9‑S12 (Weeks 19‑26)** | Phase P2 – Transformer Ranker<br>• Fine‑tune 200 M transformer<br>• A/B test vs XGBoost baseline<br>• Deploy to 20 % of orgs | Chin‑Yew (PM), Gaurav (ML) | Lift in click‑through rate ≥ 12 % over baseline, GPU ≤ 5 % of allocation |
| **S13‑S14 (Weeks 27‑30)** | Phase P3 – Explainability LLM<br>• Collect 20 K explanation pairs<br>• RLHF loop for “why” generation<br>• UI integration (hover tooltip) | Pratibha (UX), Qingwei (LLM) | Explanation correctness ≥ 80 % (human audit), latency < 150 ms |
| **S15‑S16 (Weeks 31‑34)** | Phase P4 – Federated Org‑wide Learning<br>• Secure weight‑aggregation service<br>• First cross‑tenant update<br>• Validate privacy guarantees | Dongmei (ML‑Ops), Legal | No raw data leaves tenant, weight diff ≤ 0.01 % per update |
| **S17 (Weeks 35‑36)** | Beta‑to‑GA launch<br>• Release to 100 % of tenants<br>• Admin control panel<br>• ROI dashboard | PM, Marketing, Support | Adoption ≥ 70 % after 4 weeks, trust score ≥ 80 % |
| **S18‑S22 (Weeks 37‑46)** | Post‑launch optimization<br>• Continuous RLHF from user feedback<br>• Quarterly privacy audit<br>• GPU‑usage monitoring & cost‑optimisation | All leads | GPU utilization < 5 %, cost per active user < $0.03/mo, no privacy incidents |

*Total duration: ≈ 10 months from green‑light to GA, well within the FY‑26 planning horizon.*

---

## 8. Governance & Cross‑Team Collaboration  

| Governance Body | Frequency | Decision Scope | Representative |
|-----------------|-----------|----------------|----------------|
| Steering Committee | Monthly | Budget, scope changes, risk escalation | SLT |
| Technical Review Board | Bi‑weekly | Model architecture, GPU allocation, privacy safeguards | Engineering |
| User‑Trust Council | Monthly | UI/UX guidelines, explainability standards, feedback loop design | Product |
| Compliance & Privacy Office | Quarterly | Data‑handling policies, audit outcomes | Legal/Compliance |

---

## 9. ROI & Business Impact  

| KPI | Baseline (Q4 2024) | Target (Q4 2025) | Incremental Value |
|-----|-------------------|------------------|-------------------|
| Average productive time per user | 5 h/day (post‑meeting) | 5 h + 15 min | +$12 M (2 M users, $100 / user yr) |
| Outlook triage clicks | 30 clicks/shift | 22 clicks/shift | +$4 M |
| Adoption of Copilot features | 45 % | 70 % | +$8 M |
| GPU cost | $0 | $0.03 / active‑user / mo | $0.72 M annual |
| Trust/CSAT | 72 % satisfied | ≥ 85 % | Lower churn, higher NPS (+0.6 points) |

**Total incremental ARR ≈ $24 M for FY‑26.**

---

## 10. How This Wins Rajesh, Pratibha, and Dongmei  

| Stakeholder | Primary Concern | How the Proposal Satisfies It |
|-------------|-----------------|--------------------------------|
| **Rajesh (AI Strategy)** | Clear thesis, resource feasibility, concrete scenario | Thesis: “Personal priority modeling = remediation + differentiation.”<br>GPU plan: ≤ 45 GPU‑hours Y1, phased & cost‑aware.<br>Scenario: Priority Calendar with measurable ROI. |
| **Pratibha (Product)** | User value, adoption, trust | AI‑native UX with explainable ranking, feedback loops, privacy‑by‑design – all proven to lift trust and adoption. |
| **Dongmei (Engineering / ML Ops)** | Scalable architecture, low operational load | Light‑weight XGBoost baseline → transformer upgrade only when ROI justifies; federated learning avoids raw‑data movement; GPU usage < 5 % of capacity; Spot‑VM training to keep costs down. |

---

## 11. Call to Action  

1. Approve the charter & GPU quota (≤ 2 GPUs per region).  
2. Allocate pilot budget for LLM‑assisted labeling (≈ $120 k).  
3. Form the cross‑functional team using the RACI matrix above.  
4. Set the first Steering Committee meeting (target: Week 3).  

With these steps, we can launch a high‑impact, trust‑centric Priority Calendar that demonstrably improves productivity, differentiates Microsoft 365 Copilot, and stays safely within our GPU constraints.

---

## Appendices  

### A. Glossary  

- **PPM** – Personal Priority Model (ranking transformer).  
- **RLHF** – Reinforcement Learning from Human Feedback.  
- **Federated Learning** – Model updates computed locally, aggregated securely.  
- **Explainability Layer** – Tiny LLM that produces “why this matters” snippets.  

### B. Reference Architecture Diagram  
*(See attached PDF)*  

### C. Sample UI Mock‑ups  
Focused view, Explanation tooltip, Trust Dashboard.  

### D. Privacy‑Impact Assessment Checklist  
Completed – attached.  

---  

*Prepared by: Chin‑Yew, Lead, Calendar AI*  
*Date: 23 Oct 2025*  

Let’s empower every user to focus on what truly matters—one AI‑driven priority at a time.
