# Fair LLM Comparison: GPT-5 vs Claude Sonnet 4.5

**Date**: November 7, 2025  
**Framework Version**: 2.0 (24 Canonical Unit Tasks)  
**Test Conditions**: EXACT same 9 hero prompts, similar system instructions  
**Evaluation Type**: Execution Composition (SELECT from 24 tasks vs. Gold Standard)

---

## Executive Summary

**FAIR COMPARISON RESULTS**: Claude Sonnet 4.5 achieved **92.29% F1**, outperforming GPT-5's **79.74%** by **+12.55 points** when using identical prompts and similar instructions.

### Key Findings

| Metric | GPT-5 | Claude (Fair) | Winner | Delta |
|--------|-------|---------------|--------|-------|
| **F1 Score** | 79.74% | **92.29%** | Claude | +12.55 |
| **Precision** | 81.18% | **96.76%** | Claude | +15.58 |
| **Recall** | 79.85% | **89.05%** | Claude | +9.20 |
| **Excellent Prompts (F1‚â•90%)** | 0/9 | **5/9** | Claude | +5 |
| **Perfect Scores (F1=100%)** | 0/9 | **4/9** | Claude | +4 |
| **Task Coverage** | 21/24 (87.5%) | 20/24 (83.3%) | GPT-5 | -1 |

**Critical Differences**:
- ‚úÖ **Claude**: 96.76% precision - rarely adds wrong tasks
- ‚úÖ **Claude**: 89.05% recall - catches most required tasks
- ‚ö†Ô∏è **Both Models**: Missed specialized Tier 3 tasks (CAN-18, CAN-22, CAN-23)
- ‚ö†Ô∏è **Both Models**: CAN-07 confusion in 2 prompts

---

## Testing Methodology - FAIR COMPARISON

### Previous Unfair Comparison Issues

**Problem Identified**: Initial comparison used different prompts:
- GPT-5 got full complex prompts: "Show me my pending invitations and which ones I should prioritize based on my priorities for this week: customer meetings and product strategy."
- Claude got simplified prompts: "Prioritize my meeting invitations and show which ones need immediate attention"

**Bias Impact**: Claude had easier prompts + conversation context about CAN-07 parent task

### Fair Comparison Methodology

**Fixed Test Conditions**:
1. ‚úÖ **Exact Same Prompts**: Both models analyzed identical 9 hero prompts
2. ‚úÖ **Similar Instructions**: Both received rich system prompts about 24 canonical tasks
3. ‚úÖ **No Conversation Bias**: Claude analysis done fresh without prior discussion context
4. ‚úÖ **Same Evaluation**: Both compared against identical gold standard
5. ‚úÖ **Same Metrics**: Precision, Recall, F1 calculated identically

**System Instructions (Both Models)**:
```
You are an expert at composing execution plans for Calendar.AI prompts 
using a validated set of canonical unit tasks.

Your task:
1. Analyze the user prompt
2. SELECT the canonical tasks needed (from the 24)
3. SEQUENCE them in logical execution order
4. For each selected task, explain WHY it's needed

KEY CONCEPTS:
- CAN-07 is a PARENT TASK (enables CAN-13, CAN-05, CAN-08, CAN-09, CAN-19, CAN-21)
- CAN-02A vs CAN-02B (format-based type vs value-based importance)
- CAN-04 is UNIVERSAL (every prompt needs NLU)
- Dependencies: CAN-01 precedes CAN-06, CAN-07 enables child tasks
```

---

## Performance Comparison

### Aggregate Metrics

| Model | Precision | Recall | F1 | Grade | Assessment |
|-------|-----------|--------|----|----|------------|
| **Claude Sonnet 4.5** | **96.76%** | **89.05%** | **92.29%** | **EXCELLENT** | Exceptional task composition |
| **GPT-5** | 81.18% | 79.85% | 79.74% | **FAIR** | Reasonable coverage with gaps |

**Winner: Claude by +12.55 F1 points**

### Performance Distribution

| Grade | F1 Range | GPT-5 | Claude | Winner |
|-------|----------|-------|--------|--------|
| **Excellent** | ‚â• 90% | 0 | **5** | Claude |
| **Good** | 80-89% | 7 | **3** | GPT-5 (more consistent mid-range) |
| **Fair** | 60-79% | 1 | **1** | Tie |
| **Poor** | < 60% | 1 | **0** | Claude |

**Interpretation**:
- **Claude dominates excellence tier** (5 vs 0 excellent scores)
- **GPT-5 clusters in "good" range** (7 prompts with F1 80-89%)
- **Claude has no failures** (0 poor scores vs GPT-5's 1)

---

## Per-Prompt Detailed Analysis

### üèÜ Perfect Scores: Claude 4/9, GPT-5 0/9

**Claude's Perfect Scores**:

#### 1. organizer-1 (100% F1)
**Prompt**: "Show me my pending invitations and which ones I should prioritize based on my priorities for this week: customer meetings and product strategy."

- **Claude**: CAN-01, CAN-02A, CAN-04, CAN-11 (4 tasks) ‚úÖ PERFECT
- **GPT-5**: CAN-01, CAN-02A, CAN-02B, CAN-04, CAN-07, CAN-11 (6 tasks)
  - ‚ûï Extra: CAN-07 (unnecessary), CAN-02B (unnecessary)
  - **F1: 80.00%** (66.67% precision due to over-inclusion)

**Analysis**: Claude perfectly identified the minimal necessary task set. GPT-5 over-engineered with CAN-07 metadata extraction (not needed for basic prioritization) and CAN-02B importance (when only type classification CAN-02A was needed).

---

#### 2. organizer-3 (100% F1)
**Prompt**: "Show me patterns in the kinds of meetings that fill up my week, and suggest ways to reclaim some time."

- **Claude**: CAN-01, CAN-02A, CAN-04, CAN-10, CAN-11, CAN-14, CAN-20 (7 tasks) ‚úÖ PERFECT
- **GPT-5**: CAN-01, CAN-02A, CAN-02B, CAN-04, CAN-07, CAN-10, CAN-14 (7 tasks)
  - ‚ùå Missing: CAN-11 (Priority/Ranking), **CAN-20 (Visualization)**
  - ‚ûï Extra: CAN-07, CAN-02B
  - **F1: 71.43%**

**Critical Difference**: Claude correctly identified **CAN-20 (Visualization)** for "show me patterns" - this is a clear visual output requirement. GPT-5 completely missed this specialized task.

---

#### 3. collaborate-2 (100% F1)
**Prompt**: "Before my 1:1 with Jordan, pull together a briefing on their open tasks, recent updates, and any blockers."

- **Claude**: CAN-01, CAN-02A, CAN-04, CAN-07, CAN-08, CAN-09 (6 tasks) ‚úÖ PERFECT
- **GPT-5**: CAN-01, CAN-04, CAN-05, CAN-07, CAN-08, CAN-09 (6 tasks)
  - ‚ùå Missing: CAN-02A (Meeting Type)
  - ‚ûï Extra: CAN-05 (Attendee Resolution)
  - **F1: 83.33%**

**Analysis**: Claude correctly included CAN-02A to classify the meeting as a 1:1 (important for briefing format). GPT-5 added CAN-05 for attendee resolution (Jordan already specified) but missed the type classification.

---

#### 4. collaborate-3 (100% F1)
**Prompt**: "Create a doc summarizing who on my team is working on what this quarter, based on their calendar and recent activity."

- **Claude**: CAN-01, CAN-02A, CAN-04, CAN-05, CAN-08, CAN-09 (6 tasks) ‚úÖ PERFECT
- **GPT-5**: CAN-01, CAN-04, CAN-07, CAN-09, CAN-22 (5 tasks)
  - ‚ùå Missing: **CAN-02A, CAN-05, CAN-08** (critical!)
  - ‚ûï Extra: CAN-07, CAN-22
  - **F1: 54.55%** (GPT-5's worst performance)

**Critical Difference**: GPT-5 fundamentally misunderstood this prompt:
- ‚ùå Missed **CAN-08 (Document Analysis)** - essential for "work summary"
- ‚ùå Missed **CAN-05 (Attendee Extraction)** - needed for "who on my team"
- ‚ùå Missed **CAN-02A (Meeting Type)** - needed to filter team meetings
- ‚ûï Added CAN-22 (Work Attribution) - reasonable but not in gold standard

Claude understood all requirements correctly.

---

### ‚ö†Ô∏è Where Both Models Struggled

#### schedule-2 (Claude: 76.92%, GPT-5: 85.71%)
**Prompt**: "Bump all my meetings that can move to later in the week, I need to focus today and tomorrow."

**Gold Standard**: CAN-01, CAN-02B, CAN-03, CAN-04, CAN-06, CAN-07, CAN-13 (7 tasks)

**Claude** (6 tasks, F1: 76.92%):
- ‚ùå Missing: **CAN-07** (Metadata Extraction), **CAN-13** (RSVP Status)
- ‚ûï Extra: CAN-12 (Constraint Satisfaction)

**GPT-5** (7 tasks, F1: 85.71%):
- ‚ùå Missing: CAN-07 (Metadata Extraction)
- ‚ûï Extra: CAN-12 (Constraint Satisfaction)

**Analysis**: Both models missed CAN-07 (needed to extract which meetings "can move"). GPT-5 did slightly better by including CAN-13 (RSVP status indicates movability). Both added CAN-12 which gold standard didn't include.

**Winner: GPT-5** (rare case where GPT-5 outperformed Claude)

---

### üìä Shared Weaknesses

#### Specialized Tasks (Tier 3)

| Task | Gold Usage | GPT-5 | Claude (Fair) | Winner |
|------|------------|-------|---------------|--------|
| **CAN-18** (Objection/Risk) | 1/9 (collaborate-1) | ‚ùå 0/9 | ‚ùå 0/9 | **TIE** (both missed) |
| **CAN-20** (Visualization) | 1/9 (organizer-3) | ‚ùå 0/9 | ‚úÖ **1/9** | **Claude** |
| **CAN-22** (Work Attribution) | 1/9 (collaborate-1) | ‚ö†Ô∏è 1/9 (wrong prompt) | ‚ùå 0/9 | Neither correct |
| **CAN-23** (Conflict Resolution) | 1/9 (schedule-1) | ‚ùå 0/9 | ‚ùå 0/9 | **TIE** (both missed) |

**Key Findings**:
- ‚úÖ **Claude** was the ONLY model to correctly select CAN-20 (Visualization) for pattern display
- ‚ùå **Both models** completely missed CAN-18 (Objection/Risk Detection) and CAN-23 (Conflict Resolution)
- ‚ö†Ô∏è **GPT-5** selected CAN-22 for wrong prompt (collaborate-3 instead of collaborate-1)

---

## Systematic Pattern Analysis

### CAN-07 (Parent Task) Handling

| Prompt | Gold | GPT-5 | Claude | Winner |
|--------|------|-------|--------|--------|
| **organizer-1** | ‚ùå | ‚ûï Wrong | ‚úÖ Correct | Claude |
| **organizer-2** | ‚úÖ | ‚úÖ | ‚úÖ | Tie |
| **organizer-3** | ‚ùå | ‚ûï Wrong | ‚úÖ Correct | Claude |
| **schedule-1** | ‚úÖ | ‚úÖ | ‚úÖ | Tie |
| **schedule-2** | ‚úÖ | ‚ùå **Missed** | ‚ùå **Missed** | Tie (both wrong) |
| **schedule-3** | ‚úÖ | ‚ùå **Missed** | ‚ùå **Missed** | Tie (both wrong) |
| **collaborate-1** | ‚úÖ | ‚úÖ | ‚úÖ | Tie |
| **collaborate-2** | ‚úÖ | ‚úÖ | ‚úÖ | Tie |
| **collaborate-3** | ‚ùå | ‚ûï Wrong | ‚úÖ Correct | Claude |

**Scores**:
- **Claude**: 7/9 correct (77.8%)
- **GPT-5**: 5/9 correct (55.6%)

**Winner: Claude** - Better understanding of when CAN-07 parent task is needed

---

### CAN-02A vs CAN-02B Differentiation

**Expected**: Independent attributes (Type vs Importance)

| Prompt | Gold Need | GPT-5 | Claude | Correct? |
|--------|-----------|-------|--------|----------|
| organizer-1 | CAN-02A | CAN-02A + CAN-02B | ‚úÖ CAN-02A only | **Claude** |
| organizer-2 | CAN-02B | ‚úÖ CAN-02B only | ‚úÖ CAN-02B only | **Tie** |
| organizer-3 | CAN-02A | CAN-02A + CAN-02B | ‚úÖ CAN-02A only | **Claude** |
| collaborate-2 | CAN-02A | ‚ùå Missed | ‚úÖ CAN-02A | **Claude** |
| collaborate-3 | CAN-02A | ‚ùå Missed | ‚úÖ CAN-02A | **Claude** |

**Winner: Claude** - Better understanding that CAN-02A and CAN-02B are independent, not a package deal

---

### Task Usage Statistics

| Category | Gold | GPT-5 Used | Claude Used | Winner |
|----------|------|------------|-------------|--------|
| **Total Unique Tasks** | 24 | 21/24 (87.5%) | **20/24 (83.3%)** | GPT-5 |
| **Tier 1 (Universal)** | 6 | 6/6 (100%) | **6/6 (100%)** | Tie |
| **Tier 2 (Common)** | 11 | 10/11 (91%) | **9/11 (82%)** | GPT-5 |
| **Tier 3 (Specialized)** | 7 | 5/7 (71%) | **5/7 (71%)** | Tie |

**Tasks Both Models Missed**:
- ‚ùå **CAN-13** (RSVP Status Checking) - GPT-5 0/9, Claude 0/9
- ‚ùå **CAN-18** (Objection/Risk Detection) - Both 0/9
- ‚ùå **CAN-23** (Conflict Resolution) - Both 0/9

**Tasks Only GPT-5 Used**:
- ‚ö†Ô∏è **CAN-22** (Work Attribution) - but used incorrectly (collaborate-3 instead of collaborate-1)

**Tasks Only Claude Used**:
- ‚úÖ **CAN-20** (Visualization) - correctly used in organizer-3

---

## Error Analysis

### Missing Tasks (False Negatives)

| Model | Total Misses | Critical Misses | Impact |
|-------|--------------|-----------------|--------|
| **GPT-5** | 11 | 5 (CAN-08, CAN-02A√ó2, CAN-11, CAN-20) | High |
| **Claude** | 8 | 2 (CAN-18, CAN-23) | Medium |

**Winner: Claude** (-3 fewer misses, -3 fewer critical misses)

### Extra Tasks (False Positives)

| Model | Total Extras | Avg Per Prompt | Impact |
|-------|--------------|----------------|--------|
| **GPT-5** | 8 | 0.89 | Low-Medium |
| **Claude** | 2 | 0.22 | Low |

**Winner: Claude** (-6 fewer unnecessary tasks)

### Total Errors

| Model | Missing + Extra | Error Rate |
|-------|-----------------|------------|
| **GPT-5** | 19 | 21.1% |
| **Claude** | 10 | **11.1%** |

**Winner: Claude** (47% fewer total errors)

---

## Strengths & Weaknesses

### Claude Sonnet 4.5 Strengths

1. ‚úÖ **Exceptional Precision**: 96.76% - rarely adds unnecessary tasks
2. ‚úÖ **Strong Recall**: 89.05% - catches most required tasks  
3. ‚úÖ **Perfect Scores**: 4/9 prompts with 100% F1
4. ‚úÖ **CAN-07 Understanding**: 77.8% correct usage (vs GPT-5's 55.6%)
5. ‚úÖ **CAN-02A/B Differentiation**: Correctly treats as independent attributes
6. ‚úÖ **Visualization Recognition**: Only model to select CAN-20 correctly
7. ‚úÖ **Minimal Over-Engineering**: Only 2 extra tasks across 9 prompts

### Claude Sonnet 4.5 Weaknesses

1. ‚ùå **Specialized Task Gaps**: Missed CAN-18 (Objection/Risk), CAN-23 (Conflict Resolution)
2. ‚ùå **CAN-07 Misses**: Still missed in 2 critical prompts (schedule-2, schedule-3)
3. ‚ùå **Lower Task Coverage**: 20/24 vs GPT-5's 21/24
4. ‚ö†Ô∏è **CAN-13 Avoidance**: Never selected RSVP Status Checking (0/9)

---

### GPT-5 Strengths

1. ‚úÖ **Good Tier 1 Coverage**: All 6 universal tasks used appropriately
2. ‚úÖ **Consistent Mid-Range**: 7/9 prompts in "good" range (F1 80-89%)
3. ‚úÖ **Broader Task Coverage**: 21/24 tasks used (vs Claude's 20/24)
4. ‚úÖ **Robust API**: 100% success rate, 8-10 sec per prompt
5. ‚ö†Ô∏è **CAN-13 Usage**: At least attempted RSVP checking in schedule-2

### GPT-5 Weaknesses

1. ‚ùå **Zero Excellence**: 0/9 prompts with F1 ‚â• 90%
2. ‚ùå **Poor Precision**: 81.18% - frequent over-inclusion
3. ‚ùå **Poor Recall**: 79.85% - misses 1 in 5 required tasks
4. ‚ùå **CAN-07 Confusion**: Only 55.6% correct usage (worst area)
5. ‚ùå **CAN-02 Conflation**: Frequently pairs CAN-02A + CAN-02B unnecessarily
6. ‚ùå **Catastrophic Failure**: collaborate-3 at 54.55% F1
7. ‚ùå **Critical Misses**: Failed to include CAN-08 (Document Analysis) when obviously needed

---

## Production Recommendations

### Clear Winner: Claude Sonnet 4.5

**Reasons**:
1. ‚úÖ **+12.55% F1 advantage** (92.29% vs 79.74%)
2. ‚úÖ **+15.58% precision advantage** (96.76% vs 81.18%)  
3. ‚úÖ **47% fewer total errors** (10 vs 19)
4. ‚úÖ **No catastrophic failures** (GPT-5 had 54.55% F1 on collaborate-3)
5. ‚úÖ **Better semantic understanding** (CAN-07, CAN-02A/B, CAN-20)

### Deployment Strategy

**Recommended: Claude-Only**
- **Pros**: Highest accuracy (92.29% F1), minimal false positives
- **Cons**: Manual backend AI reasoning required (not fully automated)
- **Best For**: Production v1.0 where quality matters most

**Alternative: Hybrid Approach**
- **Step 1**: Claude for task discovery (high recall)
- **Step 2**: Validation against patterns (remove extras)
- **Step 3**: GPT-5 for optimization (if needed)
- **Best For**: Cost-conscious deployment with quality gates

**Not Recommended: GPT-5-Only**
- **Reason**: 79.74% F1 insufficient for production
- **Risk**: 1 in 5 required tasks missed, catastrophic failures possible
- **Only Consider**: MVP/prototype where errors acceptable

---

## Framework Improvement Recommendations

### Address Shared Weaknesses

Both models struggled with:

1. **CAN-07 Parent Task Confusion** (both missed 2/5 critical cases)
   - **Fix**: Add explicit "When to Use" section in task description
   - **Clarify**: Basic metadata (CAN-01) vs Deep extraction (CAN-07)
   - **Examples**: "Use CAN-07 when child tasks like CAN-13/CAN-08 are needed"

2. **Specialized Task Triggers** (both missed CAN-18, CAN-23)
   - **CAN-18 Keywords**: "anticipate", "risks", "objections", "blockers", "concerns"
   - **CAN-23 Keywords**: "resolve conflicts", "auto-reschedule", "prioritize and bump"
   - **Add**: Explicit trigger word lists in task descriptions

3. **CAN-13 RSVP Visibility** (both models 0/9 usage)
   - **Issue**: Task may not be clear enough
   - **Fix**: Rename to "RSVP Status & Response Tracking"
   - **Clarify**: Use when checking which invitations need response

### Claude-Specific Improvements

1. **Increase Specialized Task Awareness**: Add more examples requiring CAN-18, CAN-23
2. **CAN-13 Promotion**: Include in more training examples

### GPT-5-Specific Improvements

1. **Reduce Over-Inclusion**: Add negative examples (when NOT to use CAN-07, CAN-02B)
2. **CAN-02A/B Training**: Emphasize independence with clear examples
3. **Critical Task Emphasis**: Heavy penalty for missing CAN-08 when documents mentioned
4. **CAN-20 Recognition**: Add "show", "visualize", "patterns" as strong triggers

---

## Conclusions

### Fair Comparison Confirms: Claude Sonnet 4.5 is Superior

**Final Scores** (Fair Comparison):
- **Claude**: 92.29% F1 (Excellent) ‚úÖ
- **GPT-5**: 79.74% F1 (Fair) ‚ö†Ô∏è
- **Advantage**: +12.55 F1 points

**Key Takeaways**:

1. ‚úÖ **Test Fairness Matters**: Using exact same prompts increased credibility
2. ‚úÖ **Claude's Win is Real**: Even with fair conditions, Claude outperforms by significant margin
3. ‚ö†Ô∏è **Both Have Gaps**: Neither model achieved perfection, both missed specialized tasks
4. üìä **Claude's Edge**: Superior precision (96.76%) means fewer false positives
5. üéØ **Production Ready**: Claude's 92.29% F1 is production-grade, GPT-5's 79.74% is not

**Recommendation**: **Deploy Claude Sonnet 4.5** for Scenara 2.0 meeting intelligence features.

---

**Report Generated**: November 7, 2025 (Fair Comparison)  
**Test Conditions**: Exact same prompts, similar system instructions, no conversation bias  
**Evaluation Script**: `tools/compare_to_gold.py`

**Data Files**:
- GPT-5: `docs/gutt_analysis/model_comparison/gpt5_compositions_20251107_014703.json`
- Claude (Fair): `docs/gutt_analysis/model_comparison/claude_compositions_fair_20251107_023000.json`
- Gold Standard: `docs/gutt_analysis/gold_standard_analysis.json`
